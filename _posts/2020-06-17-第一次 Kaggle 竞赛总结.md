```
layout: post
title: '第一次 Kaggle 竞赛总结'
date: 2020-06-17
author: yuxin.wang
color: rgb(255,210,32)
cover: 'https://miro.medium.com/max/1024/1*bQYZ7fp_wcjWFwpBbcRS-Q.png'
tags: kaggle nlp PTMs
```

# 第一次 Kaggle 竞赛总结

虽然做了很长时间的 NLP，但是一直没想去打比赛，这次正好在家有时间就参加了 kaggle 的 `tweet-sentiment-extraction`。简单来讲，比赛的任务就是从 tweet 文本中抽取出能表达情感倾向的短语，这与经典 squad 的阅读理解任务如出一辙，用 `jaccard` 作为评价指标。

## EDA

由于我一直觉得 NLP 任务中不太需要手工特征，所以对于 EDA 这部分比较放松，基本上就是看看标签的分布、文本长度以及文本的口语化程度这种基础的信息。

事实证明，在比赛中弱化 EDA 的想法是错，没有充分细致的研究训练数据让我走了很多弯路，也让我没有发现训练数据中隐藏的 magic。这关键性的错误，直接导致这次竞赛的失败。

## baseline

阅读理解任务的 baseline 最经典就是 pretrained-model + linear，输入文本经过 `Roberta` 等预训练模型后，接一个 `Linear` 层来输出 `start_span_logits` 和 `end_span_logits` ，最终选择联合起来概率最大的 span 作为输出。

根据上述的模型结构，我最终选择了如下超参数

```json
transformer_model = 'roberta-base'
epochs = 3
batch_size = 32
learning_rate = 2e-5
learning_rate_scheduler = {
  "type": 'slanted_triangular',
  "num_epochs": 3,
  "cut_frac": 0.1,
}
```

baseline 在测试集上的 jaccard 是 0.712。

## 改进方向

### 联合训练

#### Sentiment task

直接的想法来源于训练数据本身给出了每个 tweet 的情感倾向，这样的标签信息不用白不用。联合的方式也相对简单，就是将 `[CLS]` 后面接一层 `Linear` 来做分类，两个任务的 loss 加权平均，之后一起反向传播即可。但实验表明没有效果。

没有效果的原因可能在于情感倾向任务本身属于情感短语抽取的底层，而预训练模型本身已经能很好的把握情感倾向了。

#### candidate span task

另一个联合训练的任务也很自然，因为本身 baseline 模型就输出了多个候选短语及相应的概率估计，我们可以在提取短语后对这些候选短语进行进一步的特征提取并重排。但是训练的初期阶段，模型找候选短语的效果很差且极不稳定，在这时就将这两个任务结合起来将导致 *loss* 剧烈抖动。为了解决这一问题，在联合这两个任务时我添加了一个 `delay` 的超参数，即在基础任务上训练了多少 `steps` 后再联合两个任务一起训练。

设计候选短语重排模型的目标时，我试验了一下三种方式：

1. 无论模型本身的输出是否包含正确的短语，即 `golden span` 。我都会将正确答案放在待重排的 span list 中，并通过交叉熵来训练。
2. 根据模型本身的输出，计算每个候选短语与正确答案之间的距离，距离最短的就是预测目标。
3. 根据模型本身的输出，计算每个候选短语与正确答案之间的距离，依距离通过 Gaussian kernel 来赋予每个候选短语相应的概率，通过 KL 散度计算 loss。

除此之外，还考虑了将联合的任务修改成两个不同的 `Model` 来分别进行，以及将候选短语的 logits 和长度作为额外特征辅助训练。

最终，选择了 `delay=20000` 以及第一种重排目标，单模型的 jaccard 为 0.714，有小幅度增长。

### smoothing loss

由于观察到训练数据的标注有大量的 noise 且特别容易过拟合，所以我设计了基于 token 距离的 smooth loss 。这和上文中 candidate span task 中试验 3 中的方式类似。距离正确的标签越近，奖励越大，越远惩罚越大，赋予概率的方式同样使用 Gaussian kernel 。

试验证明这种 smoothing loss 与交叉熵相比在最终的指标上差异不大，但是产生的候选短语的多样性有了提升，不过劣势就是模型收敛的比较慢...

### 其他预训练模型

这次所有代码特地选择 `allennlp` 的 nightly 版本 `1.0.0rc5`。之所以选择还没有正式发布的版本就是因为这一版本有个吸引人的特性：只需要改一个参数的名称，就可以在 `tansformers` 支持的预训练模型中进行无缝切换（allennlp 的愿景）。但是，天真的我又被打脸了，由于一些代码上的 bug , 在这个任务上 allennlp 只能支持 `BERT` `ALBERT` `Roberta` ... 不过也还够用了。

除了 `Roberta-base` 之外，我还测试了 `Roberta-large` `Albert-base` `Alberta-XXlarge` 以及前两天刚刚发布的 `BERTweet` 。

通过各种精细的调参，最终发现以上的模型都不如 `Roberta-base` 。

### sequence tagger

对 jaccard 这一指标进行研究后，我发现对每个输入句子的 token 进行序列标注可能是可行的。因此我实验了两种模型，简单的序列标注模型和 `CRF + beamsearch` 模型。这两种模型都源于 NER。单模型的 jaccard 为 0.705，没有超过 baseline。

### Ensemble

1. 所有同种 tokenizer 的模型进行 token base 的概率加权平均。
2. 对于候选短语中的每一个 token 进行加权平均。

## 反思

做的好的地方在于以下几点：

1. 大胆的选择了 allennlp nightly，虽然没达到预期但是确实加快了我的实验进度。手动实现一个魔改的序列标注模型只用了我不到 1 个小时的时间，实验各种特殊的 loss 和超参设定让我只需要修改配置文件。虽然，我想在这个任务中测试 `BART` 和 `ELECTRA` 的愿望没有实现...
2. 一开始想过是否要对数据进行预处理，不过观察了训练数据大量的噪声后，果断放弃了这一枯燥的工作。事实证明我预想的预处理方式不能带来模型提升。
3. 通过调研果断放弃了数据增强，NLP 的数据增强在数据量充足的情况下基本可以判死刑。最先进的基于上下文和预训练模型的数据增强论文也没能证明数据增强的效果，反而都是在取巧的方向上做实验，以便发论文。
4. 手写了大量代码，对 `torch` 的使用更加得心应手，能快速的实验各种灵感。

5. 训练了 50+ 的模型，对调参有了更多的心得。

做的不好的地方在于以下几点：

1. 论文还是看的少了，早点看到 [ALBERT Premium for SQuAD 2.0](https://www.semanticscholar.org/paper/ALBERT-Premium-for-SQuAD-2.0-Dhareshwar-Jiang/7b5d0906c09c133ddba25b1709a6aa90f23b1840) 这一篇，就不会走这么多弯路了。不过看到斯坦福的同学也和我一样在错误的方向上疯狂尝试，心里的挫败感少了许多。
2. 探索的方向没有科学的指导，灵感来了就写代码做实验了... 太盲目了。如果能有一个更加科学的规划，会让整个探索更高效。
3. 没有重视 EDA 和 ensemble，这两者直接导致我这倒霉成绩。
4. 没有更好的进行实验记录，否则可以水一篇失败尝试集锦，就像 ALBERT Premium for SQuAD 2.0 一样，哈哈。
5. 在预训练模型的基础上做模型架构层面的改进，难度是以前的几倍，千万慎重！这部分我的思路还是太天真了，需要追追最前沿的 paper 了。

## 失败总结

虽然是第一次参加比赛，但投入的精力和财力还是很多的，最后的结果一度使我很沮丧。总结起来，这次的失败既是天灾也是人祸。

天灾在于比赛本身存在问题，发现训练数据秘密的队伍可以轻松拿到银牌及以上，这是由于竞赛本身的失误造成的。不过话说回来，我观察到了数据中存在严重的噪声，却没有推理出这些噪声是由于 **空格数量导致的** ，福尔摩斯看的少了。

人祸是因为我自身知识水平的局限，在 ensemble 的技巧和认识上存在短板。之前一直在公司训练模型，响应时间往往是需要首先考虑的因素，所以基本上都不做 ensemble。这使得我所有关于 ensemble 的认识都来源于书本，这让我在模型选择的判断上产生了巨大失误。通过看了其他同学的解决方案和竞赛论文后，我修正了这一方面的知识。单模型性能差不一定是坏事，差异性才是关键。

如果我在以上两方面中的任何一方面做的足够好，拿奖牌都是顺理成章的。

不过得之我幸，失之我命，尽自己最大努力就好，下一次比赛目标还是保银争金，自勉！

