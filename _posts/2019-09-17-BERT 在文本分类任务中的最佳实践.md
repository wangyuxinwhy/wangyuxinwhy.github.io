---
layout: post
title: 'BERT 在文本分类任务中的最佳实践'
date: 2019-09-17
author: yuxin.wang
color: rgb(255,210,32)
cover: 'https://img2018.cnblogs.com/blog/1214565/201901/1214565-20190116111955500-1615019895.png'
tags: bert text-classification
---

# BERT 在文本分类任务中的最佳实践

本文为 [How to Fine-Tune BERT for Text Classification?](https://i.loli.net/2019/09/17/vZptAxPqE2IydFQ.png) 的学习笔记，旨在探索使用 BERT 进行文本分类的最佳实践。后续的笔记就是对此 paper 的浓缩，着急的同学可以直接看下面的个人经验以及最后的结论。

BERT 在使用中如果有领域内的语料，一定要进行 Further Pretraining。这将极大的提高模型的性能。在 Further Pretraining 的时候注意学习率以及训练步数的选择，不要让模型产生灾难性遗忘。Further Pretraining 之后，我们就可以使用得到的 BERT 模型在具体的下游任务进行 Fine-tune 了，这里注意不要使用 Feature based 方法自己嫁接新的结构，这么做被证明是对性能有伤害的。Fine-tune 的时候同样要注意超参数的选择及模型输出（只使用最后一层的输出）的选择，面对特别长的文本的时候使用 head+tail 的方式进行 truncate 即可。

## BERT 使用框架

![](https://i.loli.net/2019/09/17/vZptAxPqE2IydFQ.png)

看起来是三种路径，实际上就是是否进行 Further Pretraining 和是否进行多任务学习。

## Hyperparameters

### Further Pretraining

```json
{
  "batch_size": 32,
  "max_seq_len": 128,
  "learning_reate": 5e-5,
  "train_steps": 100 * 1000,
  "warm_up_steps": 10 * 1000
}
```

### Fine-tune

```json
{
  "batch_size": 24,
  "dropout_prob": .1,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "learning_rate_scchrduler": "slanted triangular",
  "learning_rate": 2e-5,
  "warm_up_prop": 0.1,
  "epochs": 4
}
```

## Investigating Different Fine-Tuning Strategies

### Dealing with long texts

1. Truncation methods

   - head-only：保留前 510 个字符
   - tail-only： 保留后 510 个字符
   - head+tail：保留前 128 个字符及后 382 个字符

2. Hierarchical methods

   先将长文本切割成 $K = L / 510$ 个片段，之后分别取不同片段的向量表示。通过 `MaxPolling` `MeanPolling` `self-attention` 的方式进行归约

![image-20190917153443492](https://i.loli.net/2019/09/17/T4WSOo7Gf8qhsje.png)

最后分别在两个数据中进行实验，结果表明 **head+tail** 展示出了最佳的性能。

### Features from Different layers

在文本分类任务中，我们应该选择哪些层的 `hidden states` 作为特征？实验主要集中在测试`BERT` 中那些层及其结合方式最有效。 

![](https://i.loli.net/2019/09/17/vLbVmc97XnBOWAI.png)

实验在 IMDb 数据集中进行，最后显示**只使用最后一层**的输出取得了最佳性能。

### Catastrophic Forgetting

根据我对灾难性遗忘的理解，没看到这篇 paper 的实验...

总之告诫我们使用 Bert 的时候尽量使用小一点的学习率。$2e-5$ 或者 $5e-5$

![](https://i.loli.net/2019/09/17/Ti69C1WIsLwvOmG.png)

### Layer-wise Decreasing Layer Rate

我们要不要对不同的层使用不同的学习率呢？直觉上来说，底层的层需要更多的捕捉细节的信息，所以需要更低的学习率来不断找到最优解。而顶层的层则更直接与任务相关，所以需要较高的学习率来加速学习。

最终实验的结论是，可能有这样的调整会更好。但是性能没有非常大的提高，所以还是算了，就用统一的学习率就好。

## Investigating the Further Pretraining

BERT 毕竟是在开放的领域进行训练的，然而我们具体的任务是局限在一个单一的领域。比如说影评、保险问题等等。开放领域与具体的单一领域的数据分布不完全相同，为了让 BERT 更好的适应具体的下游任务，往往需要让 BERT 在**训练集**、**领域内文本**或者**交叉领域文本**中再进行一定步数的 Further Pretraining 。大量的实验证明这样做是很有意义的，并且在以上三种选择中往往在 **领域内（in-domain）** 语料中进行 Further Pretraining 会取得最佳性能！

### Within-Task Further Pre-Training

使用训练集进行 Further Pretraining 时，训练的步数太少达不到效果，太多会造成灾难性遗忘。因此需要选择合适的步数才行，paper 中的实验表明了这一点。

![](https://i.loli.net/2019/09/17/Sfwu5rCchMjmlnx.png)

不同的训练数据集的字符规模往往不同，如果不是特别大的数据集，看起来选择 $100K$ 作为训练步数是合理的。

### In-Domain and Cross-Domain Further Pre-Training

![](https://i.loli.net/2019/09/17/SpiWhPY5vLylCaK.png)

在 **领域内（in-domain）** 语料中进行 Further Pretraining 会取得最佳性能！

### Comparisons to Previous Models

BERT 的作者将使用预训练的模型分为两种，一种是 ELMo 的 Feature-base 型和 BERT 的 Fine-tune 型。也就是说，我们应该把预训练模型的输出作为特征之后输入到其他任意架构的模型中，还是在原有的与训练模型架构中做任务适应型的 Fine-tune 呢？

**对于 BERT 来讲可能 Fine-tune 的形式可能更好！**

![](https://i.loli.net/2019/09/17/Gno9UB2MXqJjDOv.png)

我们可以从图中看到，BERT-ITPT-FiT 及 BERT-IDPT-FiT 的优势极为明显，也就是说我们在之后的文本分类任务中集中测试这两种模型就可以了。

## Multi-task Fine-Tuning

多任务学习不是很关注，先省略掉。肯定是有效果的，不过多任务学习的实验往往需要很精细的调整，所以不想浪费太多的精力在这个上面。

## Few-Shot Learning

因为 BERT 已经学习了语义、句法等相关知识，所以我们可以利用这一点来进行小样本学习（few-shot lerning）。

![](https://i.loli.net/2019/09/17/1KZUhzVYvmMIpse.png)

图中展示了只使用 $0.4 \%$ 的数据就可以实现 $9.23\% $ 的错误率，后续的 $96\%$ 只让模型性能提高了 $5\%$ 而已。

## Further Pre-Training on BERT Large

BERT Large 和 BERT base 在性质上相似，以上结论可以推广。

## Conclusion

1. The top layer of BERT is more useful for text classification
2. With an appropriate layer-wise decreasing learning rate, BERT can overcome the catastrophic forgetting problem
3. Within-task and in-domain further pre-training can significantly boost its performance

4. A preceding multi-task fine-tuning is also helpful to the single-task fine-tuning, but its benefit is smaller than further pre-training
5. BERT can improve the task with small-size data